{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718265d8-be19-4bf8-9046-0a34f423f8a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "if ! [[ \"18.04 20.04 22.04 23.04 24.04\" == *\"$(lsb_release -rs)\"* ]];\n",
    "then\n",
    "    echo \"Ubuntu $(lsb_release -rs) is not currently supported.\";\n",
    "    exit;\n",
    "fi\n",
    "\n",
    "# Add the signature to trust the Microsoft repo\n",
    "# For Ubuntu versions < 24.04 \n",
    "curl https://packages.microsoft.com/keys/microsoft.asc | sudo tee /etc/apt/trusted.gpg.d/microsoft.asc\n",
    "# For Ubuntu versions >= 24.04\n",
    "curl https://packages.microsoft.com/keys/microsoft.asc | sudo gpg --dearmor -o /usr/share/keyrings/microsoft-prod.gpg\n",
    "\n",
    "# Add repo to apt sources\n",
    "curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list\n",
    "\n",
    "# Install the driver\n",
    "sudo apt-get update\n",
    "sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18\n",
    "# optional: for bcp and sqlcmd\n",
    "sudo ACCEPT_EULA=Y apt-get install -y mssql-tools18\n",
    "echo 'export PATH=\"$PATH:/opt/mssql-tools18/bin\"' >> ~/.bashrc\n",
    "source ~/.bashrc\n",
    "# optional: for unixODBC development headers\n",
    "sudo apt-get install -y unixodbc-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2395479a-3215-461d-a189-3a13c7662f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/EDP-Engineering/Setup_Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8fb009b-656c-4fbf-aa1d-63737b851448",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DatabaseAdapter"
    }
   },
   "outputs": [],
   "source": [
    "# DatabaseAdapter class\n",
    "import re\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "name_regex = re.compile('[^A-Za-z0-9_]')\n",
    "class DatabaseAdapter(object):\n",
    "    \"\"\"\n",
    "    Handles both generic and feature specific database interactions\n",
    "    \"\"\"\n",
    "\n",
    "    def _split_conn_string(conn_string: str):\n",
    "        retval = {}\n",
    "        if conn_string != None and len(conn_string) > 0 and \";\" in conn_string and \"=\" in conn_string:\n",
    "            # parts = conn_string.split(';')\n",
    "            recs = [x.split('=', 1)\n",
    "                    for x in conn_string.split(';') if len(x) > 0]\n",
    "            retval = dict([(key.lower().strip(), value.strip()) for key, value in recs])\n",
    "        return retval\n",
    "\n",
    "    def __init__(self, dbConnString: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize an instance of the adapter with an ODBC connection string\n",
    "        \n",
    "        params: \n",
    "        dbConnString: str - A ODBC compliant connection string\n",
    "        \"\"\"\n",
    "        if dbConnString is None:\n",
    "            raise ValueError(\n",
    "                \"dbConnString was empty but must contain a valid database connection string\")\n",
    "        self.connString = dbConnString\n",
    "        # self.jdbcjarPath=jdbcjarPath\n",
    "        connDict = DatabaseAdapter._split_conn_string(dbConnString)\n",
    "        self.driver = connDict['driver']\n",
    "\n",
    "        if \"server\" in connDict.keys():\n",
    "            server_parts = connDict['server'].split(',')\n",
    "        else:\n",
    "            server_parts = connDict['data source'].split(',')\n",
    "        self.server = server_parts[0].replace('tcp:', '')\n",
    "        if len(server_parts) > 1:\n",
    "            port = server_parts[1]\n",
    "        else:\n",
    "            port = \"1433\"\n",
    "        self.database = connDict['database']\n",
    "        self.uid = connDict['uid']\n",
    "        if 'port' in connDict.keys() and len(connDict['port']) > 0:\n",
    "            self.port = connDict['port']\n",
    "        else:\n",
    "            self.port = 1433\n",
    "\n",
    "        self.conn: pyodbc.Connection = None  # self.__openDBConn()\n",
    "\n",
    "    def __openDBConn(self):\n",
    "        try:\n",
    "            if (self.conn is not None):\n",
    "                return self.conn\n",
    "\n",
    "            conn = pyodbc.connect(self.connString, autocommit=True)\n",
    "            self.conn = conn\n",
    "            return self.conn\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Couldn't connect to %s:%s with user %s\", self.server, self.database, self.uid, exc_info=e)\n",
    "            raise ConnectionError(\n",
    "                \"Database connection has failed. Please review!\")\n",
    "\n",
    "    def getConn(self):\n",
    "        return self.__openDBConn()\n",
    "\n",
    "    def closeDBConn(self):\n",
    "        if self.conn != None:\n",
    "            self.conn.close()\n",
    "        self.conn = None\n",
    "\n",
    "    def __del__(self):\n",
    "        self.closeDBConn()\n",
    "\n",
    "    def query_results_fetchall(self, queryStatement: str, closeConn=False):\n",
    "        \"\"\"\n",
    "        Execute the provided query in the metadata repository\n",
    "        :param queryStatement: the sql query/statement to be executed\n",
    "        :param db_conn_dict: a dictionary containing values required to connect to the database. \n",
    "                             Required keys are: driver,url,password,user,databaseName\n",
    "        :return: return an array of results\n",
    "        \"\"\"\n",
    "        if queryStatement is None or len(queryStatement) == 0:\n",
    "            raise ValueError(\"queryStatement was empty but is required\")\n",
    "\n",
    "        try:\n",
    "            if (self.conn == None):\n",
    "                self.__openDBConn()\n",
    "\n",
    "            curs = self.conn.cursor()\n",
    "            curs.execute(queryStatement)\n",
    "            return_objects = curs.fetchall()\n",
    "            curs.close()\n",
    "            return return_objects\n",
    "        except Exception as e:\n",
    "            errmsg = 'An error occured while attempting query: {}.'.format(\n",
    "                queryStatement)\n",
    "            logger.error(errmsg, exc_info=e)\n",
    "            if (self.conn != None):\n",
    "                self.conn.close()\n",
    "                self.conn = None\n",
    "            raise\n",
    "        finally:\n",
    "            if closeConn:\n",
    "                self.closeDBConn()\n",
    "\n",
    "    def query_results_fetchone(self, queryStatement: str, closeConn=False):\n",
    "        \"\"\"\n",
    "        Execute the provided query in the metadata repository\n",
    "        :param queryStatement: the sql query/statement to be executed\n",
    "        :param db_conn_dict: a dictionary containing values required to connect to the database. \n",
    "                             Required keys are: driver,url,password,user,databaseName\n",
    "        :return: return an array of results\n",
    "        \"\"\"\n",
    "        if queryStatement is None or len(queryStatement) == 0:\n",
    "            raise ValueError(\"queryStatement was empty but is required\")\n",
    "\n",
    "        try:\n",
    "            if (self.conn == None):\n",
    "                self.__openDBConn()\n",
    "\n",
    "            curs = self.conn.cursor()\n",
    "            curs.execute(queryStatement)\n",
    "            return_objects = curs.fetchone()\n",
    "            curs.close()\n",
    "            return return_objects\n",
    "        except Exception as e:\n",
    "            errmsg = 'An error occured while attempting query: {}.'.format(\n",
    "                queryStatement)\n",
    "            logger.error(errmsg, exc_info=e)\n",
    "            if (self.conn != None):\n",
    "                self.conn.close()\n",
    "                self.conn = None\n",
    "            raise\n",
    "        finally:\n",
    "            if closeConn:\n",
    "                self.closeDBConn()\n",
    "\n",
    "    def execute_query(self, queryStatement: str, closeConn=False):\n",
    "        \"\"\"\n",
    "        Execute the provided query in the metadata repository\n",
    "        :param queryStatement: the sql query/statement to be executed\n",
    "        :param db_conn_dict: a dictionary containing values required to connect to the database. \n",
    "                             Required keys are: driver,url,password,user,databaseName\n",
    "        :return: return an array of results\n",
    "        \"\"\"\n",
    "        if queryStatement is None or len(queryStatement) == 0:\n",
    "            raise ValueError(\"queryStatement was empty but is required\")\n",
    "\n",
    "        try:\n",
    "            if (self.conn == None):\n",
    "                self.__openDBConn()\n",
    "\n",
    "            curs = self.conn.cursor()\n",
    "            if len(queryStatement.strip()) > 0:  # exclude new lines\n",
    "                curs.execute(queryStatement)\n",
    "                curs.nextset()  # added 08212023 Bug#40624\n",
    "\n",
    "            curs.close()\n",
    "        except Exception as e:\n",
    "            errmsg = 'An error occured while attempting query: {}.'.format(\n",
    "                queryStatement)\n",
    "            logger.error(errmsg, exc_info=e)\n",
    "            if (self.conn != None):\n",
    "                self.conn.close()\n",
    "                self.conn = None\n",
    "            raise\n",
    "        finally:\n",
    "            if closeConn:\n",
    "                self.closeDBConn()\n",
    "            \n",
    "    def execute_query_batch(self, queryStatements: str, closeConn=False):\n",
    "        \"\"\"\n",
    "        Executes the Batch of queries separated by 'GO\\n'  TSQL applicable only        \n",
    "        \"\"\"\n",
    "\n",
    "        if queryStatements is None or len(queryStatements.strip()) == 0:\n",
    "            raise ValueError(\"queryStatement was empty but is required\")\n",
    "\n",
    "        if (self.conn == None):\n",
    "            self.__openDBConn()\n",
    "        curs = self.conn.cursor()\n",
    "        sqlbatchlist = queryStatements.encode(\n",
    "            \"ascii\", errors=\"ignore\").decode().split('GO\\n')\n",
    "        try:\n",
    "            for sqlstmt in sqlbatchlist:\n",
    "                try:\n",
    "                    if len(sqlstmt.strip()) > 0:  # exclude new lines\n",
    "                        curs.execute(sqlstmt)\n",
    "                except Exception as e:\n",
    "                    errmsg = 'An error occured while attempting query: {}.'.format(\n",
    "                        sqlstmt)\n",
    "                    logger.error(errmsg, exc_info=e)\n",
    "                    raise\n",
    "        finally:\n",
    "            curs.close()\n",
    "            if closeConn:\n",
    "                self.closeDBConn()\n",
    "\n",
    "    def getDF(self, selectSQL):\n",
    "        if (self.conn is None):\n",
    "            self.__openDBConn()\n",
    "        df = pd.read_sql(selectSQL, self.conn)  # to be removed\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52b0d8b-e5b1-4283-af5c-3eba622963b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Install the required package\n",
    "%pip install opencensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "609e9dd6-b526-4a49-b841-605d2c492364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_select_statement(source_name: str, table_name: str):\n",
    "    sql = f\"\"\"SELECT DISTINCT COLUMN_NAME, DATA_TYPE\n",
    "              FROM INFORMATION_SCHEMA.COLUMNS\n",
    "              WHERE TABLE_NAME = '{table_name}'\n",
    "              AND TABLE_SCHEMA = '{source_name}_base'\"\"\"\n",
    "    df = synapse_adapter.getDF(sql)\n",
    "    \n",
    "\n",
    "    casts = []\n",
    "    for row in df.itertuples():\n",
    "        if row.DATA_TYPE.lower() == \"uniqueidentifier\":\n",
    "            cast_query = f\"CAST({row.COLUMN_NAME} AS VARCHAR(36)) AS {row.COLUMN_NAME}\"\n",
    "        else:\n",
    "            cast_query = f\"{row.COLUMN_NAME}\"\n",
    "        casts.append(cast_query)\n",
    "\n",
    "    select_clause = \", \".join(casts)\n",
    "    return select_clause\n",
    "\n",
    "def create_external_synapse_table(source_name: str, table: str):\n",
    "    location = f'edp/migration/{source_name.lower()}/{table.lower()}/'\n",
    "    create_schema_statement = f\"\"\"if schema_id('migration_{source_name}_ext') is NULL\n",
    "    \t    exec sp_executesql N'create schema [migration_{source_name}_ext]'\"\"\"\n",
    "    synapse_adapter.execute_query(create_schema_statement)\n",
    "\n",
    "    drop_statement = f\"\"\"if object_id('[migration_{source_name}_ext].[{table}]') is not null\n",
    "    \tdrop external table [migration_{source_name}_ext].[{table}]\"\"\"\n",
    "    synapse_adapter.execute_query(drop_statement)\n",
    "\n",
    "    dbutils.fs.rm(f\"/mnt/{location}\", recurse=True)\n",
    "    \n",
    "    logger.info(\"Creating external table for %s_base.%s\", source_name, table)\n",
    "    create_statement = f\"\"\"CREATE EXTERNAL TABLE [migration_{source_name}_ext].[{table}] WITH (\n",
    "                LOCATION = '{location}',\n",
    "                DATA_SOURCE = [AzureDataLakeStore_mainstore],\n",
    "                FILE_FORMAT = [parquetfile_snappy]\n",
    "        ) AS\n",
    "        SELECT\n",
    "            {create_select_statement(source_name, table)}\n",
    "        FROM [{source_name}_base].[{table}]\"\"\"\n",
    "\n",
    "    #execute sql on Synapse\n",
    "    synapse_adapter.execute_query(create_statement)\n",
    "\n",
    "#%restart_python\n",
    "import os\n",
    "import logging\n",
    "from ELTFramework.Library.globals import environment\n",
    "logger = logging.getLogger(\"migration\")\n",
    "logger.setLevel(logging.INFO)\n",
    "print(environment)\n",
    "#setup environment variables\n",
    "if environment == \"dev\":\n",
    "        edpHostname = \"sen-edp-dev-sdw-01.database.windows.net\"\n",
    "        edpDatabase = \"sen-edp-sdw-sqlpool-01\"\n",
    "        catalog = \"edp_dev\"\n",
    "        adls_storage_account = \"sedpdevedpadls01\"\n",
    "        keyVaultName = \"seedpdevedpadmkv01\"\n",
    "elif environment == \"test\":\n",
    "        edpHostname = \"edpsdwqa.database.windows.net\"\n",
    "        edpDatabase = \"edpsdwqa\"\n",
    "        catalog = \"edp_test\"\n",
    "        keyVaultName = \"seedptestedpadmkv01\"\n",
    "elif environment == \"prod\":\n",
    "        edpHostname = \"edpsdwprod.database.windows.net\"\n",
    "        edpDatabase = \"edpsdwprod\"\n",
    "        catalog = \"edp\"\n",
    "        keyVaultName = \"seedpprodedpadmkv01\"\n",
    "secrets_scope = \"primary-kv\"\n",
    "\n",
    "edpsdw_etl_user = dbutils.secrets.get(scope=secrets_scope, key=\"etl-user-name\")\n",
    "edpsdw_etl_password = dbutils.secrets.get(scope=secrets_scope, key=\"etl-user-pass\")\n",
    "edpsdw_conn_string = f\"Driver={{ODBC Driver 18 for SQL Server}};Server=tcp:{edpHostname},1433;Database={edpDatabase};Uid={edpsdw_etl_user};Pwd={edpsdw_etl_password};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
    "synapse_adapter = DatabaseAdapter(edpsdw_conn_string)\n",
    "edpprocmgmt_conn_string = f\"Driver={{ODBC Driver 18 for SQL Server}};Server=tcp:{edpHostname},1433;Database=EDPProcMgmt;Uid={edpsdw_etl_user};Pwd={edpsdw_etl_password};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
    "edpprocmgmt_adapter = DatabaseAdapter(edpprocmgmt_conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "014eebc6-5246-42f5-891a-0677477e6225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import current_user, col, to_utc_timestamp\n",
    "from pyspark.sql.types import StringType,DateType,TimestampType\n",
    "import json\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "\n",
    "#get the list of tables to pull by trigger if provided, or using the array of objects\n",
    "event_trigger_name = dbutils.widgets.get(\"event_trigger_name\")\n",
    "if len(event_trigger_name) > 0 and event_trigger_name != 'None':\n",
    "    tables_to_migrate = edpprocmgmt_adapter.query_results_fetchall(f\"select object_metadata_id, source_name, table_name from ETL.vActiveTablesToLoad where Event_Trigger_Name='{event_trigger_name}' and Sdw_Load_Strategy <> 'truncate'\")\n",
    "    keys = ['object_metadata_id', 'source_name', 'table_name']\n",
    "    tables_to_migrate = [dict(zip(keys, item)) for item in tables_to_migrate]\n",
    "\n",
    "else:\n",
    "    tables_to_migrate = dbutils.widgets.get(\"objects_to_process\")\n",
    "    logger.info(\"trying to parse %s from data type %s\", tables_to_migrate, type(tables_to_migrate))\n",
    "    if len(tables_to_migrate) > 0 and tables_to_migrate != 'None':\n",
    "        tables_to_migrate = json.loads(tables_to_migrate)\n",
    "    else:\n",
    "        raise ValueError(\"Expecting either event_trigger_name or objects_to_process to be pass to this notebook.\")\n",
    "\n",
    "#create an external table in synapse\n",
    "for table in tables_to_migrate:\n",
    "    table_exported = False\n",
    "    object_metadata_id = table['object_metadata_id']\n",
    "    source_name = table['source_name'].lower()\n",
    "    table_name = table['table_name'].lower()\n",
    "    try:\n",
    "        create_external_synapse_table(source_name, table_name)\n",
    "        table_exported = True\n",
    "        # metadata_adapter.log_table_load_starting(source_name, '', table['table'])\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error creating external table for %s.%s\", source_name, table_name, exc_info=e)\n",
    "\n",
    "    # Retrieve original column types from Synapse\n",
    "    query = f\"\"\"\n",
    "        SELECT column_name, data_type\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = '{source_name}_base' AND table_name = '{table_name}'\n",
    "    \"\"\"    \n",
    "    columns = synapse_adapter.query_results_fetchall(query)\n",
    "    # Identify UNIQUEIDENTIFIER columns\n",
    "    uniqueidentifier_columns = {column_name.lower() for column_name, data_type in columns if data_type.lower() == \"uniqueidentifier\"} \n",
    "    # Identify DATE columns\n",
    "    date_columns = {column_name.lower() for column_name, data_type in columns if data_type.lower() == \"date\"}      \n",
    "\n",
    "    rows_written_parquet = 0\n",
    "    #load the data to Unity Catalog\n",
    "    try:\n",
    "        table_loaded = False\n",
    "        if table_exported:\n",
    "            df = spark.read.parquet(f\"/mnt/edp/migration/{source_name}/{table_name}/\")\n",
    "             # Build the list of columns with transformations defined based on the data type\n",
    "            transformed_columns = []\n",
    "            for field in df.schema.fields:\n",
    "                field_name = field.name.lower()\n",
    "                field_type = field.dataType.simpleString()\n",
    "\n",
    "\n",
    "                if field_name in date_columns:\n",
    "                    if field_type == \"timestamp\":\n",
    "                        # Convert TIMESTAMP back to DATE\n",
    "                        transformed_columns.append(col(field_name).cast(DateType()).alias(field_name))\n",
    "                        \n",
    "                    else: \n",
    "                        transformed_columns.append(col(field_name))\n",
    "\n",
    "                elif field_type ==\"timestamp\":\n",
    "                    # Convert from Eastern Time to UTC\n",
    "                    transformed_columns.append(\n",
    "                        to_utc_timestamp(col(field_name), \"America/New_York\").alias(field_name)\n",
    "                    )\n",
    "                elif field_name in uniqueidentifier_columns:  \n",
    "                    transformed_columns.append(col(field_name).cast(StringType()).alias(field_name))         \n",
    "                else:\n",
    "                    transformed_columns.append(col(field_name))\n",
    "\n",
    "            # Select all transformed columns at once to optimize the DataFrame transformation\n",
    "            df = df.select(*transformed_columns)\n",
    "            create_schema_sql = f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{source_name}_base\"\n",
    "            spark.sql(create_schema_sql)\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"True\").saveAsTable(f\"{catalog}.{source_name}_base.{table_name}\")\n",
    "            table_loaded = True\n",
    "            rows_written_parquet = df.count()\n",
    "            logger.info(\"wrote %d rows to %s.%s_base.%s\", rows_written_parquet, catalog, source_name, table_name)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error creating table %s.%s_base.%s\", catalog, source_name, table_name, exc_info=e)\n",
    "\n",
    "    try:\n",
    "        if table_loaded:\n",
    "            edpprocmgmt_adapter.execute_query(f\"insert into dataops.Databricks_Bronze_Migration_Tracker (Object_Metadata_Id,Source_Row_Count,Target_Row_Count,Schema_Match,Data_Match,Row_Count_Match,Test_Status) values({object_metadata_id}, null, {rows_written_parquet}, 0, 0, 0, 'initial extract from synapse complete')\")\n",
    "        else:\n",
    "            edpprocmgmt_adapter.execute_query(f\"insert into dataops.Databricks_Bronze_Migration_Tracker (Object_Metadata_Id,Source_Row_Count,Target_Row_Count,Schema_Match,Data_Match,Row_Count_Match,Test_Status) values({object_metadata_id}, null, {rows_written_parquet}, 0, 0, 0, 'initial extract from synapse failed')\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error writing to tracker for table %s.%s_base.%s\", catalog, source_name, table_name, exc_info=e)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3149472327624519,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Initial_Load_From_Synapse",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
