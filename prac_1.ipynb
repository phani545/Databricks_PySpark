{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6187e579-e06d-462b-9873-af311aa00747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#Union() : combaining two or more DataFrames which is having same sturture and schema\n",
    "#UnionByName() : combaining two or more DataFrames having different schema and takes param #allowMissingColumns = True\n",
    "\n",
    "ame1 = spark.createDataFrame([(\"bob\",42),(\"lisa\",32)],[\"first name\",\"age\"])\n",
    "col1 = spark.createDataFrame([(\"maria\",20),(\"camlin\",35)],[\"first name\",\"age\"])\n",
    "ame1.show()\n",
    "col1.show()\n",
    "\n",
    "result = ame1.union(col1)\n",
    "result.show()\n",
    "\n",
    "\n",
    "details = [(1,'krishna','IT','male')]\n",
    "column = ['id','name','department','gender']\n",
    "\n",
    "details1 = [(1,'krishna','IT','1000')]\n",
    "column1 = ['id','name','department','salary']\n",
    "\n",
    "\n",
    "df1 = spark.createDataFrame(details,column)\n",
    "df2 = spark.createDataFrame(details1,column1)\n",
    "df1.show()\n",
    "df2.show()\n",
    "\n",
    "df3 = df1.union(df2)\n",
    "df3.show()\n",
    "\n",
    "df1.unionByName(df2,allowMissingColumns=True).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40926009-24bc-4122-b0a1-b99da2cd3809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#Eample 2\n",
    "dataframe1 = spark.createDataFrame([('James',87.33),('Anna',93.25),('Anita',87.48)],['name','OverallPercentage'])\n",
    "dataframe2 = spark.createDataFrame([('Maria1',94.43),('Anna1',94.53),('Anita1',94.48)],['name','OverallPercentage'])\n",
    "dataframe1.show()\n",
    "dataframe2.show()\n",
    "\n",
    "dataframe1.union(dataframe2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c0e995f-0b98-40d5-8a36-47b1707315ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#UnionBYName: combaine two or more dataframes which have doffernce schema because its conbaine data by name not by order of column\n",
    "d1 = spark.createDataFrame([(\"maria\",20),(\"camlin\",35)],[\"first name\",\"age\"])\n",
    "df2 = spark.createDataFrame([(20,\"maria\"),(35,\"camlin\")],[\"age\",\"first name\"])\n",
    "d1.show()\n",
    "df2.show()\n",
    "\n",
    "d1.unionByName(df2,allowMissingColumns=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b785e6c7-ee62-4d27-b7d8-850d9e8e0800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number,tag\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [('James','Sales',18,90000),('Michael','Sales',24,86000),\n",
    "  ('Robert','Sales',32,81000),('Maria','Finance',34,90000),\n",
    "  ('Raman','Finance',31,99000),('Scott','Finance',33,83000),\n",
    "  ('Jen','Finance',29,79000)]\n",
    "columns = [\"employee_name\",\"department\",\"age\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()\n",
    "windowpart = Window.partitionBy(\"department\").orderBy(\"age\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "display(df)\n",
    "df_with_rownum = df.withColumn(\"row_number\", row_number().over(windowpart))\n",
    "df_with_rownum.show()\n",
    "       \n",
    "#using rank(): rank()\n",
    "#using cume_dist(): cume_dist() function is used to get the cumulative distribution within a window partition\n",
    "\n",
    "from pyspark.sql.functions import cume_dist, row_number\n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowpart)).show()\n",
    "\n",
    "#using lag() function is used to access the previous rows data as per defined offset value in the function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "709bb9ea-955f-4d47-8a69-13344990ef41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "prac_1",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
